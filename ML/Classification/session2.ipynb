{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is SVM?\n",
    "\n",
    "SVM (Support Vector Machine) is a type of machine learning algorithm used mainly for classification. Classification means categorizing data into different classes or groups.\n",
    "\n",
    "Imagine you have a set of points on a 2D plane, and you want to separate them into two different categories (for example, red points and blue points). SVM helps you find the best line (or boundary) that divides these two groups of points.\n",
    "\n",
    "## How does it work?\n",
    "\n",
    "### Separating the data:\n",
    "SVM tries to find a line (in 2D) or a plane (in higher dimensions) that best divides the two classes. This line is called a **hyperplane**.\n",
    "\n",
    "### Maximizing the margin:\n",
    "The key idea behind SVM is to make sure that the line (hyperplane) is placed in such a way that it has the largest possible distance (**margin**) between the closest points from each class. These closest points are called **support vectors**. So, SVM focuses on the points that are hardest to classify, and it uses these points to decide where to place the boundary.\n",
    "\n",
    "### Linear or Non-linear:\n",
    "\n",
    "- **Linear SVM**: If the data can be easily separated by a straight line (or flat surface in higher dimensions), SVM will find that.\n",
    "- **Non-linear SVM**: If the data cannot be separated by a straight line (for example, if the points form a circle or other complex shape), SVM can use something called a **kernel trick** to transform the data into a higher-dimensional space where it can be separated by a straight line.\n",
    "\n",
    "## How does SVM choose the best line?\n",
    "SVM’s goal is to place the line in the middle of the gap between the two classes, ensuring that it maximizes the margin (the distance between the closest data points of each class and the line). This way, the classifier is more likely to correctly classify new data points in the future.\n",
    "\n",
    "## What is a support vector?\n",
    "The **support vectors** are the data points that are closest to the hyperplane. These points are important because they are the ones that \"support\" or determine the position of the dividing line.\n",
    "\n",
    "## What if the data isn't perfectly separable?\n",
    "In real life, the data might not always be perfectly separated by a line (or plane). SVM can still handle this by allowing some points to be on the wrong side of the line. This is done by introducing a **soft margin**, which allows some mistakes but still tries to keep the margin as wide as possible. The **penalty parameter** \\( C \\) helps control how many mistakes the algorithm is allowed to make.\n",
    "\n",
    "## Advantages of SVM:\n",
    "- **Effective in high-dimensional spaces**: SVM works well when there are many features or dimensions in your data.\n",
    "- **Good at handling complex data**: It can handle non-linear data using kernels.\n",
    "- **Works well with small to medium-sized datasets**.\n",
    "\n",
    "## Disadvantages of SVM:\n",
    "- **Computationally expensive**: It can take a long time to train the model on large datasets.\n",
    "- **Difficult to interpret**: The results aren’t as easy to interpret as some other algorithms (like decision trees).\n",
    "\n",
    "## Summary:\n",
    "SVM is a powerful tool for classifying data by finding the best boundary (line or plane) that separates different classes. It works by maximizing the margin between the classes and can handle both simple linear cases and more complex non-linear cases using kernels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>MajorAxisLength</th>\n",
       "      <th>MinorAxisLength</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>ConvexArea</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87524</td>\n",
       "      <td>442.246011</td>\n",
       "      <td>253.291155</td>\n",
       "      <td>0.819738</td>\n",
       "      <td>90546</td>\n",
       "      <td>0.758651</td>\n",
       "      <td>1184.040</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75166</td>\n",
       "      <td>406.690687</td>\n",
       "      <td>243.032436</td>\n",
       "      <td>0.801805</td>\n",
       "      <td>78789</td>\n",
       "      <td>0.684130</td>\n",
       "      <td>1121.786</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90856</td>\n",
       "      <td>442.267048</td>\n",
       "      <td>266.328318</td>\n",
       "      <td>0.798354</td>\n",
       "      <td>93717</td>\n",
       "      <td>0.637613</td>\n",
       "      <td>1208.575</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Area  MajorAxisLength  MinorAxisLength  Eccentricity  ConvexArea  \\\n",
       "0  87524       442.246011       253.291155      0.819738       90546   \n",
       "1  75166       406.690687       243.032436      0.801805       78789   \n",
       "2  90856       442.267048       266.328318      0.798354       93717   \n",
       "\n",
       "     Extent  Perimeter    Class  \n",
       "0  0.758651   1184.040  Kecimen  \n",
       "1  0.684130   1121.786  Kecimen  \n",
       "2  0.637613   1208.575  Kecimen  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"D:\\\\utils\\\\DataSets\\\\Raisin_Dataset.xlsx\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"Area\", \"MajorAxisLength\", \"MinorAxisLength\", \"Eccentricity\", \"ConvexArea\", \"Extent\", \"Perimeter\"]]\n",
    "y = df[\"Class\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Besni       0.86      0.75      0.80        83\n",
      "     Kecimen       0.81      0.90      0.85        97\n",
      "\n",
      "    accuracy                           0.83       180\n",
      "   macro avg       0.83      0.82      0.82       180\n",
      "weighted avg       0.83      0.83      0.83       180\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([229], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(kernel=\"rbf\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "model.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Besni       0.91      0.88      0.90        83\n",
      "     Kecimen       0.90      0.93      0.91        97\n",
      "\n",
      "    accuracy                           0.91       180\n",
      "   macro avg       0.91      0.90      0.90       180\n",
      "weighted avg       0.91      0.91      0.91       180\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([85005907], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "#You can notice that when you use RBF kernel, the number of iterations is less but linear takes more iterations to converge.\n",
    "#make sure before running you have that computation power to run the linear kernel.\n",
    "\n",
    "model = SVC(kernel=\"linear\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "model.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Max Scaling (Normalization)\n",
    "\n",
    "Min-max scaling, also known as min-max normalization, is a data preprocessing technique used to transform numerical data within a specific range.\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "* It's primarily used to bring all numerical features in a dataset to a common scale. This is important because many machine learning algorithms perform better when numerical input variables are scaled to a standard range.\n",
    "* This is especially helpful for algorithms that are sensitive to the magnitude of features, such as:\n",
    "    * K-nearest neighbors (KNN)\n",
    "    * Support vector machines (SVM)\n",
    "    * Neural networks\n",
    "\n",
    "**How it Works:**\n",
    "\n",
    "* The process involves linearly transforming the original data.\n",
    "* Typically, the data is scaled to a range between 0 and 1.\n",
    "* The formula used is:\n",
    "    * `X_scaled = (X - X_min) / (X_max - X_min)`\n",
    "        * Where:\n",
    "            * `X_scaled` is the scaled value.\n",
    "            * `X` is the original value.\n",
    "            * `X_min` is the minimum value of the feature.\n",
    "            * `X_max` is the maximum value of the feature.\n",
    "\n",
    "**Key Considerations:**\n",
    "\n",
    "* **Sensitivity to Outliers:** Min-max scaling is sensitive to outliers. If your data contains extreme values, they can significantly affect the scaling, leading to a compressed range for the majority of the data.\n",
    "* **Preservation of Distribution:** It's important to understand that min-max scaling does not change the distribution of the data; it simply rescales it.\n",
    "\n",
    "In essence, min-max scaling is a valuable tool for ensuring that all features contribute equally to the learning process of a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaler (Standardization)\n",
    "\n",
    "Standard scaler, or standardization, is another common data preprocessing technique used to transform numerical data. Unlike min-max scaling, standard scaler focuses on transforming data to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "* It aims to center the data around zero and scale it to unit variance.\n",
    "* This is crucial for algorithms that assume data is normally distributed or those sensitive to feature variances (e.g., support vector machines, linear regression with regularization).\n",
    "* It can help improve the stability and performance of machine learning models.\n",
    "\n",
    "**How it Works:**\n",
    "\n",
    "* Standard scaler transforms the data by subtracting the mean and dividing by the standard deviation.\n",
    "* The formula is:\n",
    "\n",
    "    $$\n",
    "    X_{scaled} = \\frac{X - \\mu}{\\sigma}\n",
    "    $$\n",
    "\n",
    "    * Where:\n",
    "        * $X_{scaled}$ is the scaled value.\n",
    "        * $X$ is the original value.\n",
    "        * $\\mu$ is the mean of the feature.\n",
    "        * $\\sigma$ is the standard deviation of the feature.\n",
    "\n",
    "**Key Considerations:**\n",
    "\n",
    "* **Less Sensitive to Outliers:** Standard scaler is generally less sensitive to outliers than min-max scaling because it uses the mean and standard deviation, which are less affected by extreme values. However, very large outliers can still have some influence.\n",
    "* **Preserves Distribution (Approximately):** While it doesn't guarantee a perfect normal distribution, it attempts to center the data and scale it to a standard deviation of 1, which can make the data more closely resemble a standard normal distribution.\n",
    "* **Unbounded Range:** The scaled values from standard scaler are not confined to a specific range (like 0 to 1 in min-max scaling). They can be positive or negative and have values beyond the typical -3 to +3 range, particularly if there are outliers.\n",
    "* **When to Use:** Use standard scaler when your data is approximately normally distributed, or when your model assumes normality. It is also good practice when you are unsure which scaling method to use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaler.fit(X)\n",
    "\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "standard_scaler.fit(X)\n",
    "\n",
    "\n",
    "X_train_mm = min_max_scaler.fit_transform(X_train)\n",
    "X_test_mm = min_max_scaler.fit_transform(X_test)\n",
    "\n",
    "X_train_ss = standard_scaler.fit_transform(X_train)\n",
    "X_test_ss = standard_scaler.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Besni       0.82      0.87      0.84        83\n",
      "     Kecimen       0.88      0.84      0.86        97\n",
      "\n",
      "    accuracy                           0.85       180\n",
      "   macro avg       0.85      0.85      0.85       180\n",
      "weighted avg       0.85      0.85      0.85       180\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([219], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scaling using min max scaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(kernel=\"rbf\")\n",
    "model.fit(X_train_mm, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_mm)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "model.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Besni       0.78      0.87      0.82        83\n",
      "     Kecimen       0.88      0.79      0.83        97\n",
      "\n",
      "    accuracy                           0.83       180\n",
      "   macro avg       0.83      0.83      0.83       180\n",
      "weighted avg       0.83      0.83      0.83       180\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([196], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scaling using min max scaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(kernel=\"linear\")\n",
    "model.fit(X_train_mm, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_mm)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "model.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Besni       0.88      0.86      0.87        83\n",
      "     Kecimen       0.88      0.90      0.89        97\n",
      "\n",
      "    accuracy                           0.88       180\n",
      "   macro avg       0.88      0.88      0.88       180\n",
      "weighted avg       0.88      0.88      0.88       180\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([382], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scaling using standard scaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(kernel=\"rbf\")\n",
    "model.fit(X_train_ss, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_ss)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "model.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Besni       0.85      0.84      0.85        83\n",
      "     Kecimen       0.87      0.88      0.87        97\n",
      "\n",
      "    accuracy                           0.86       180\n",
      "   macro avg       0.86      0.86      0.86       180\n",
      "weighted avg       0.86      0.86      0.86       180\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1214], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scaling using standard scaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(kernel=\"linear\")\n",
    "model.fit(X_train_ss, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_ss)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "model.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Besni       0.91      0.83      0.87        83\n",
      "     Kecimen       0.87      0.93      0.90        97\n",
      "\n",
      "    accuracy                           0.88       180\n",
      "   macro avg       0.89      0.88      0.88       180\n",
      "weighted avg       0.88      0.88      0.88       180\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1214], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Easy process\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm\", SVC())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "model.n_iter_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
